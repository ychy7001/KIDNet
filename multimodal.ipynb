{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Block Index\n",
    "\n",
    "- [roi_extract](#roi_extract)\n",
    "- [scaler](#scaler)\n",
    "- [split_dataset](#split_dataset)\n",
    "- [train](#train)\n",
    "- [test](#test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"roi_extract\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550cdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi_extract\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 路径设置\n",
    "image_dir = \"./toy-dataset/breast_bm_b-mode/images\"  # 原始图像路径\n",
    "label_dir = \"./toy-dataset/breast_bm_b-mode/labels\"  # JSON标注路径\n",
    "output_dir = \"./toy-dataset/images\"  # 裁剪后保存路径\n",
    "os.makedirs(output_dir, exist_ok=True)  # 创建输出目录\n",
    "\n",
    "# 遍历所有JSON文件\n",
    "for json_file in os.listdir(label_dir):\n",
    "    if json_file.endswith(\".json\"):\n",
    "        json_path = os.path.join(label_dir, json_file)\n",
    "        \n",
    "        # 读取JSON标注\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 获取矩形坐标 (x1, y1), (x2, y2)\n",
    "        points = data[\"shapes\"][0][\"points\"]\n",
    "        x1, y1 = points[0]  # 左上角\n",
    "        x2, y2 = points[1]  # 右下角\n",
    "        \n",
    "        # 打开原始图像并裁剪\n",
    "        image_name = data[\"imagePath\"].split(\"/\")[-1]  # 如 \"B-4-CAIWENZHI-1.jpg\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # 确保坐标在图像范围内\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(img.width, x2)\n",
    "        y2 = min(img.height, y2)\n",
    "        \n",
    "        # 裁剪并保存\n",
    "        cropped_img = img.crop((x1, y1, x2, y2))\n",
    "        output_path = os.path.join(output_dir, image_name)\n",
    "        cropped_img.save(output_path)\n",
    "\n",
    "print(f\"已完成所有图像裁剪，保存在 {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scaler\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe448df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 路径设置\n",
    "input_csv_path = \"./toy-dataset/features_raw.csv\"\n",
    "output_csv_path = \"./toy-dataset/features.csv\"\n",
    "\n",
    "# 1. 读取原始CSV文件\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：找不到文件 {input_csv_path}\")\n",
    "    exit()  # 停止程序\n",
    "\n",
    "# 2. 检查必须存在的列\n",
    "required_columns = [\"anonymous_id\", \"label\"]\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    print(f\"错误：CSV文件缺少必要的列。必须包含 {required_columns}\")\n",
    "    exit()  # 停止程序\n",
    "\n",
    "\n",
    "# 3. 分离ID和标签列\n",
    "id_col = df[\"anonymous_id\"]\n",
    "label_col = df[\"label\"]\n",
    "feature_cols = [col for col in df.columns if col not in [\"anonymous_id\", \"label\"]]\n",
    "\n",
    "# 4. 提取需要归一化的特征\n",
    "features_df = df[feature_cols]\n",
    "\n",
    "\n",
    "# 5. 使用MinMaxScaler进行归一化\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(features_df)\n",
    "\n",
    "# 6. 创建归一化后的DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_features, columns=feature_cols)\n",
    "\n",
    "# 7. 合并ID、归一化后的特征和标签\n",
    "final_df = pd.concat([id_col, normalized_df, label_col], axis=1)\n",
    "\n",
    "# 8. 保存到新的CSV文件\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"特征已归一化并保存到: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split_dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os\n",
    "\n",
    "# 路径设置\n",
    "input_csv_path = \"./toy-dataset/features.csv\"\n",
    "output_csv_path = \"./toy-dataset/features_split.csv\"\n",
    "\n",
    "# 划分比例\n",
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15 # TRAIN_SIZE + VAL_SIZE + TEST_SIZE 应该接近 1.0\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = os.path.dirname(output_csv_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"创建输出目录: {output_dir}\")\n",
    "\n",
    "print(f\"正在读取数据集: {input_csv_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：找不到文件 {input_csv_path}\")\n",
    "    exit()\n",
    "\n",
    "# 检查是否存在 label 列用于分层抽样\n",
    "if 'label' not in df.columns:\n",
    "    print(\"错误：CSV文件缺少 'label' 列，无法进行分层抽样。\")\n",
    "    exit()\n",
    "\n",
    "print(f\"原始数据集大小: {len(df)}\")\n",
    "\n",
    "# 进行分层抽样划分\n",
    "# 先划分出测试集\n",
    "split1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=42)\n",
    "train_val_indices, test_indices = next(split1.split(df, df['label']))\n",
    "\n",
    "# 然后在剩余的训练+验证集中划分出验证集\n",
    "# 注意：这里 test_size 是相对于 train_val_indices 的比例\n",
    "# 验证集相对于总体的比例是 VAL_SIZE，那么它占 train_val_indices 的比例是 VAL_SIZE / (TRAIN_SIZE + VAL_SIZE)\n",
    "val_relative_size = VAL_SIZE / (TRAIN_SIZE + VAL_SIZE)\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=val_relative_size, random_state=42)\n",
    "train_indices, val_indices = next(split2.split(df.iloc[train_val_indices], df.iloc[train_val_indices]['label']))\n",
    "\n",
    "# 将划分结果映射回原始 DataFrame 的索引\n",
    "train_original_indices = df.iloc[train_val_indices].iloc[train_indices].index\n",
    "val_original_indices = df.iloc[train_val_indices].iloc[val_indices].index\n",
    "test_original_indices = df.iloc[test_indices].index\n",
    "\n",
    "# 在 DataFrame 中添加 'set' 列并赋值\n",
    "df['set'] = 'unknown' # 初始化\n",
    "df.loc[train_original_indices, 'set'] = 'train'\n",
    "df.loc[val_original_indices, 'set'] = 'val'\n",
    "df.loc[test_original_indices, 'set'] = 'test'\n",
    "\n",
    "print(f\"划分结果:\")\n",
    "print(df['set'].value_counts())\n",
    "\n",
    "# 检查是否有未被划分的数据\n",
    "if 'unknown' in df['set'].unique():\n",
    "     print(\"警告：存在未被划分的数据。\")\n",
    "\n",
    "# 保存划分后的 CSV 文件\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"数据集划分完成，结果已保存到: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3406bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# 超参数 (保持不变)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 224\n",
    "# Transformer 参数 (保持与之前一致)\n",
    "PATCH_SIZE = 16\n",
    "NUM_HEADS = 8\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "# 路径设置 (保持不变)\n",
    "image_dir = \"./toy-dataset/images\"\n",
    "csv_split_path = \"./toy-dataset/features_split.csv\"\n",
    "output_dir = \"./\"\n",
    "\n",
    "# 确保输出目录存在 (保持不变)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"创建输出目录: {output_dir}\")\n",
    "\n",
    "# --- Transformer 工具类 (保持不变) ---\n",
    "class PositionalEncoding(nn.Module):  # ... (保持不变)\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (sequence_length, batch_size, d_model)\n",
    "        # self.pe shape: (max_len, d_model)\n",
    "        # We need to add position encoding for the sequence length\n",
    "        pe = self.pe[:x.size(0), :].unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False) # batch_first=False aligns with PositionalEncoding expected input\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (sequence_length, batch_size, d_model)\n",
    "        src2 = self.norm1(src)\n",
    "        # MultiheadAttention expects (sequence_length, batch_size, d_model) if batch_first=False\n",
    "        src2, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout(src2)\n",
    "        return src\n",
    "\n",
    "# --- MultiModalModel 类 (修改以使用 ResNet18 作为图像特征提取器) ---\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. 使用预训练的 ResNet18 作为图像特征提取器\n",
    "        self.resnet = models.resnet18(pretrained=True)  # 下载并加载预训练权重\n",
    "\n",
    "        # 冻结 ResNet18 的参数 (可选，如果你希望只训练后面的层)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 修改 ResNet18 的最后几层，使其输出适合 Transformer 的输入\n",
    "        # 移除 ResNet18 的 AvgPool 和 FC 层\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2]) # Remove avgpool and fc layer\n",
    "\n",
    "        # 添加一个卷积层，将 ResNet18 的输出通道数调整为 HIDDEN_DIM\n",
    "        self.conv = nn.Conv2d(512, HIDDEN_DIM, kernel_size=1) # ResNet18 outputs 512 channels\n",
    "\n",
    "        # 2. 图像Patch处理\n",
    "        PATCH_KERNEL = 4\n",
    "        PATCH_STRIDE = 4\n",
    "        self.patch_embed = nn.Conv2d(HIDDEN_DIM, HIDDEN_DIM,\n",
    "                                     kernel_size=PATCH_KERNEL, stride=PATCH_STRIDE)\n",
    "        self.num_patches = (56 // PATCH_KERNEL) * (56 // PATCH_STRIDE)\n",
    "\n",
    "        # 3. 数值特征处理 (保持不变)\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Linear(num_features, HIDDEN_DIM),\n",
    "            nn.LayerNorm(HIDDEN_DIM),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # 4. Transformer部分 (保持不变)\n",
    "        self.seq_length = 1 + 1 + self.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, HIDDEN_DIM))\n",
    "        self.pos_encoder = PositionalEncoding(HIDDEN_DIM, max_len=self.seq_length)\n",
    "        self.transformer = nn.ModuleList([\n",
    "            TransformerEncoderLayer(HIDDEN_DIM, NUM_HEADS)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "\n",
    "        # 5. 分类头 (保持不变)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(HIDDEN_DIM),\n",
    "            nn.Linear(HIDDEN_DIM, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, features):\n",
    "        # 1. 使用 ResNet18 提取图像特征\n",
    "        img_feat = self.resnet(image)  # [B, 512, 7, 7] (ResNet18 before avgpool)\n",
    "        img_feat = self.conv(img_feat) # [B, HIDDEN_DIM, 7, 7]\n",
    "        img_feat = self.patch_embed(img_feat) # [B, HIDDEN_DIM, 14, 14]\n",
    "        img_feat = img_feat.flatten(2).transpose(1, 2)  # [B, num_patches, HIDDEN_DIM]\n",
    "\n",
    "        # 2. 处理数值特征 (保持不变)\n",
    "        num_feat = self.feature_proj(features).unsqueeze(1)  # [B, 1, HIDDEN_DIM]\n",
    "\n",
    "        # 3. 添加CLS token和融合 (保持不变)\n",
    "        cls_tokens = self.cls_token.expand(image.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, num_feat, img_feat], dim=1)  # [B, 1+1+num_patches, HIDDEN_DIM]\n",
    "\n",
    "        # 4. Transformer编码 (保持不变)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "\n",
    "        # 5. 分类 (保持不变)\n",
    "        cls_output = x[0]  # [B, HIDDEN_DIM]\n",
    "        output = self.classifier(cls_output)\n",
    "        return output\n",
    "\n",
    "# --- 数据集类 (保持不变) ---\n",
    "class MultiModalDataset(Dataset):  # ... (保持不变)\n",
    "    def __init__(self, csv_path, image_dir, set_type, transform=None):\n",
    "        df_full = pd.read_csv(csv_path)\n",
    "        # 根据 set_type 过滤数据\n",
    "        if set_type not in ['train', 'val', 'test']:\n",
    "            raise ValueError(\"set_type must be 'train', 'val', or 'test'\")\n",
    "        self.df = df_full[df_full['set'] == set_type].reset_index(drop=True) # Reset index after filtering\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.set_type = set_type\n",
    "        \n",
    "        # Cache feature column names\n",
    "        self.feature_cols = [col for col in self.df.columns if col not in [\"anonymous_id\", \"label\", \"set\"]]\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} samples for set_type: {self.set_type}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取当前行的数据\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 加载结构化特征\n",
    "        features = row[self.feature_cols].values.astype(float)\n",
    "        \n",
    "        # 加载标签\n",
    "        label = row[\"label\"].astype(int)\n",
    "        \n",
    "        # 加载图像 (匿名化后名称格式为 img_XXX.jpg)\n",
    "        # 注意：这里的 idx 是过滤后 DataFrame 的行索引，可能与原始 CSV 的索引不同\n",
    "        # 因此需要存储原始索引或 anonymous_id 来找到对应的图片文件。\n",
    "        # 假设匿名化后的名称格式为 img_[原始索引].jpg\n",
    "        # 如果你的图片命名是 img_000.jpg, img_001.jpg 等直接对应处理后的CSV行索引，\n",
    "        # 那么需要确保features_split.csv保存时保留了原始顺序或者有映射关系。\n",
    "        # 考虑到 split_dataset.py 只是添加了 set 列并保存，它保留了原始索引顺序。\n",
    "        # 因此，filtered_df.iloc[idx] 对应的图片索引应该是其原始索引。\n",
    "        # 查找原始索引: df_full[df_full['set'] == set_type].index[idx]\n",
    "        # 或者更简单：如果在 MultiModalDataset.__init__ 中 reset_index(drop=True)\n",
    "        # 那么新的 DataFrame 的 index 0, 1, 2... 对应的是过滤后的行的顺序。\n",
    "        # 如果 img_XXX.jpg 是根据原始 CSV 的行号匿名化的，我们需要找到当前行在原始 features.csv 中的位置。\n",
    "        # split_dataset.py 没有删除或重排序原始行，只是加了一列。\n",
    "        # 所以 features_split.csv 的第 i 行仍然对应 img_i.jpg。\n",
    "        # 然而，过滤后的 self.df 的第 i 行可能不是原始 CSV 的第 i 行。\n",
    "        # 最安全的方法是假设 anonymous_id 或其他唯一标识符可以映射到图片文件名。\n",
    "        # 但是你的匿名化代码 img_XXX.jpg 似乎是按顺序来的。\n",
    "        # 如果 img_XXX.jpg 对应的是 *原始* features_raw.csv 或 features.csv 的行号 X,\n",
    "        # 那么你需要找到当前 row 在原始 features.csv 中的位置。\n",
    "        # split_dataset.py 保留了原始索引。所以 self.df 的 index 属性就是原始索引。\n",
    "        original_index = self.df.index[idx]\n",
    "        image_name = f\"img_{original_index:03d}.jpg\" # 使用原始索引来构建文件名\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "             print(f\"警告: 找不到图片文件 {image_path}，跳过此样本。\")\n",
    "             # 返回一个占位符或者处理错误，这里简单返回None，需要在DataLoader中处理或过滤\n",
    "             # 更健壮的方法是在__init__时检查图片是否存在，并从df中移除对应的行\n",
    "             # 但为了代码简洁，这里先这样处理\n",
    "             return None\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"features\": torch.FloatTensor(features),\n",
    "            \"label\": torch.LongTensor([label]).squeeze()\n",
    "        }\n",
    "\n",
    "# --- 数据加载 (创建训练和验证DataLoader) ---\n",
    "# 计算特征数量 (保持不变)\n",
    "try:\n",
    "    df_temp = pd.read_csv(csv_split_path)\n",
    "    num_features = len([col for col in df_temp.columns if col not in [\"anonymous_id\", \"label\", \"set\"]])\n",
    "    print(f\"检测到特征数量: {num_features}\")\n",
    "    del df_temp\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：找不到划分后的数据集文件 {csv_split_path}。请先运行 split_dataset.py\")\n",
    "    exit()\n",
    "\n",
    "# 2. 图像预处理 (使用与 ResNet18 预训练时相同的 Normalization 参数)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet mean and std\n",
    "])\n",
    "\n",
    "train_dataset = MultiModalDataset(csv_split_path, image_dir, set_type='train', transform=transform)\n",
    "val_dataset = MultiModalDataset(csv_split_path, image_dir, set_type='val', transform=transform)\n",
    "\n",
    "# --- 定义 collate_fn 处理可能存在的 None (由于图片缺失) ---\n",
    "def collate_fn(batch): # ... (保持不变)\n",
    "    # 过滤掉 None 样本\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None # 如果整个批次都是 None，返回 None\n",
    "\n",
    "    # 将字典列表转换为字典，其中每个值是张量批次\n",
    "    return {key: torch.stack([d[key] for d in batch]) for key in batch[0]}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# --- 训练和验证函数 (保持不变) ---\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device): # ... (保持不变)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # 处理空批次\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        images = batch[\"image\"].to(device)\n",
    "        features = batch[\"features\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images, features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device): # ... (保持不变)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # 处理空批次\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            images = batch[\"image\"].to(device)\n",
    "            features = batch[\"features\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images, features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# --- 主训练流程 (保持不变) ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    model = MultiModalModel(num_features=num_features, num_classes=NUM_CLASSES).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_epoch = -1\n",
    "    model_save_path = os.path.join(output_dir, 'best_model.pth')\n",
    "\n",
    "    print(\"开始训练...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "        val_loss, val_accuracy = validate_epoch(model, val_dataloader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # 保存最佳模型 (基于验证集损失)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"  >> 验证集损失改进 ({best_val_loss:.4f})，保存模型到 {model_save_path}\")\n",
    "        # 也可以选择基于验证集准确率保存最佳模型\n",
    "        # if val_accuracy > best_val_accuracy:\n",
    "        #     best_val_accuracy = val_accuracy\n",
    "        #     best_val_loss = val_loss # 记录对应的损失\n",
    "        #     best_epoch = epoch + 1\n",
    "        #     torch.save(model.state_dict(), model_save_path)\n",
    "        #     print(f\"  >> 验证集准确率改进 ({best_val_accuracy:.4f})，保存模型到 {model_save_path}\")\n",
    "\n",
    "    print(\"\\n训练完成！\")\n",
    "    print(f\"最佳模型保存在 Epoch {best_epoch}，验证集损失为: {best_val_loss:.4f}，验证集准确率为: {best_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 超参数 (需要与训练时保持一致，尤其是模型相关的参数)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 224\n",
    "# Transformer 参数\n",
    "PATCH_SIZE = 16\n",
    "NUM_HEADS = 8\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "# 路径设置\n",
    "image_dir = \"./toy-dataset/images\"              # 裁剪后图像路径\n",
    "csv_split_path = \"./toy-dataset/features_split.csv\" # 划分后的CSV路径\n",
    "model_path = \"./best_model.pth\"    # 训练保存的最佳模型权重路径\n",
    "\n",
    "# --- Transformer 工具类 (与train.py保持一致) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:x.size(0), :].unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "        return x + pe\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=False)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.norm1(src)\n",
    "        src2, _ = self.self_attn(src2, src2, src2)\n",
    "        src = src + self.dropout(src2)\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout(src2)\n",
    "        return src\n",
    "\n",
    "# --- MultiModalModel 类 (与train.py保持一致) ---\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1), # Output: [B, 64, 56, 56]\n",
    "            nn.Conv2d(64, HIDDEN_DIM, kernel_size=3, stride=1, padding=1, bias=False), # Output: [B, HIDDEN_DIM, 56, 56]\n",
    "            nn.BatchNorm2d(HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        PATCH_KERNEL = 4\n",
    "        PATCH_STRIDE = 4\n",
    "        self.patch_embed = nn.Conv2d(HIDDEN_DIM, HIDDEN_DIM,\n",
    "                                     kernel_size=PATCH_KERNEL, stride=PATCH_STRIDE)\n",
    "        self.num_patches = (56 // PATCH_KERNEL) * (56 // PATCH_STRIDE)\n",
    "\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Linear(num_features, HIDDEN_DIM),\n",
    "            nn.LayerNorm(HIDDEN_DIM),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.seq_length = 1 + 1 + self.num_patches # CLS + Numerical + Patches\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, HIDDEN_DIM))\n",
    "        self.pos_encoder = PositionalEncoding(HIDDEN_DIM, max_len=self.seq_length)\n",
    "        self.transformer = nn.ModuleList([\n",
    "            TransformerEncoderLayer(HIDDEN_DIM, NUM_HEADS)\n",
    "            for _ in range(NUM_LAYERS)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(HIDDEN_DIM),\n",
    "            nn.Linear(HIDDEN_DIM, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, features):\n",
    "        img_feat = self.cnn(image)\n",
    "        img_feat = self.patch_embed(img_feat)\n",
    "        img_feat = img_feat.flatten(2).transpose(1, 2)\n",
    "\n",
    "        num_feat = self.feature_proj(features).unsqueeze(1)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(image.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, num_feat, img_feat], dim=1)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "\n",
    "        cls_output = x[0]\n",
    "        output = self.classifier(cls_output)\n",
    "        return output\n",
    "\n",
    "# --- 自定义数据集类 (与train.py保持一致, 过滤 set_type='test') ---\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, set_type, transform=None):\n",
    "        df_full = pd.read_csv(csv_path)\n",
    "        if set_type not in ['train', 'val', 'test']:\n",
    "            raise ValueError(\"set_type must be 'train', 'val', or 'test'\")\n",
    "        self.df = df_full[df_full['set'] == set_type].reset_index(drop=True)\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.set_type = set_type\n",
    "        \n",
    "        self.feature_cols = [col for col in self.df.columns if col not in [\"anonymous_id\", \"label\", \"set\"]]\n",
    "        \n",
    "        print(f\"Loaded {len(self.df)} samples for set_type: {self.set_type}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        features = row[self.feature_cols].values.astype(float)\n",
    "        label = row[\"label\"].astype(int)\n",
    "        \n",
    "        # Use original index from the filtered dataframe's index attribute\n",
    "        original_index = self.df.index[idx]\n",
    "        image_name = f\"img_{original_index:03d}.jpg\"\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "             print(f\"警告: 找不到图片文件 {image_path}，跳过此样本。\")\n",
    "             return None\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"features\": torch.FloatTensor(features),\n",
    "            \"label\": torch.LongTensor([label]).squeeze()\n",
    "        }\n",
    "\n",
    "# 2. 图像预处理 (与train.py保持一致)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- 数据加载 (创建测试DataLoader) ---\n",
    "# 计算特征数量\n",
    "try:\n",
    "    df_temp = pd.read_csv(csv_split_path)\n",
    "    num_features = len([col for col in df_temp.columns if col not in [\"anonymous_id\", \"label\", \"set\"]])\n",
    "    print(f\"检测到特征数量: {num_features}\")\n",
    "    del df_temp\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误：找不到划分后的数据集文件 {csv_split_path}。请先运行 split_dataset.py\")\n",
    "    exit()\n",
    "\n",
    "test_dataset = MultiModalDataset(csv_split_path, image_dir, set_type='test', transform=transform)\n",
    "\n",
    "# --- collate_fn (与train.py保持一致) ---\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None\n",
    "    return {key: torch.stack([d[key] for d in batch]) for key in batch[0]}\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn) # Test set does not need shuffle\n",
    "\n",
    "\n",
    "# --- 测试函数 ---\n",
    "def test_model(model, dataloader, device):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"开始测试...\")\n",
    "    with torch.no_grad(): # Disable gradient calculation for testing\n",
    "        for batch in dataloader:\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            images = batch[\"image\"].to(device)\n",
    "            features = batch[\"features\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images, features)\n",
    "            _, predicted = torch.max(outputs, 1) # Get the index of the max log-probability\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # 计算并打印评估指标\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"\\n测试集准确率: {accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\n分类报告:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=[f'Class {i}' for i in range(NUM_CLASSES)]))\n",
    "\n",
    "    print(\"\\n混淆矩阵:\")\n",
    "    print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "\n",
    "# --- 主测试流程 ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"使用设备: {device}\")\n",
    "\n",
    "    # 实例化模型 (结构必须与训练时一致)\n",
    "    model = MultiModalModel(num_features=num_features, num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "    # 加载训练好的模型权重\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"加载模型权重: {model_path}\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    else:\n",
    "        print(f\"错误：找不到模型权重文件 {model_path}。请先运行 train.py 完成训练。\")\n",
    "        exit()\n",
    "\n",
    "    # 运行测试\n",
    "    test_model(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
